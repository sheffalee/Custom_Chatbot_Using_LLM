# -*- coding: utf-8 -*-
"""myownllm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11v95uuLSzy51NYmO_axvqbVzMexxtLRQ
"""

!pip install cohere

!pip install annoy

import cohere

import numpy as np

import warnings

warnings.filterwarnings('ignore')

from annoy import AnnoyIndex

import numpy as np

import pandas as pd

!pip install datasets

from datasets import load_dataset

dataset = load_dataset("rajpurkar/squad")

print(pd.DataFrame(dataset["train"][:10]))

question = "Where is the headquarters of the Congregation of the Holy Cross?"

text = """

The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome).

Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building.

Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians.

Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto.

The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic, Buechner has praised writers

from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.

"""

# Split into a list of paragraphs

texts = text.split('\n\n')


# Clean up to remove empty spaces and new lines

texts = np.array([t.strip(' \n') for t in texts if t])


co = cohere.Client('N7sPFSCnfRHtdGYRExOtomjhglebtXBUENi935rs')


# Get the embeddings

response = co.embed(

    texts=texts.tolist(),

).embeddings

# Check the dimensions of the embeddings

embeds = np.array(response)

# Create the search index, pass the size of embedding

search_index = AnnoyIndex(embeds.shape[1], 'angular')

# Add all the vectors to the search index

for i in range(len(embeds)):

    search_index.add_item(i, embeds[i])

search_index.build(10) # 10 trees

search_index.save('test.ann')

def search_text(query):

    # Get the query's embedding

    query_embed = co.embed(texts=[query]).embeddings

    # Retrieve the nearest neighbors

    similar_item_ids = search_index.get_nns_by_vector(query_embed[0], 10, include_distances=True)

    search_results = texts[similar_item_ids[0]]

    return search_results

results = search_text(question)

print(results[0])

def ask_llm(question, num_generations=1):

    # Search the text archive

    results = search_text(question)

    # Get the top result

    context = results[0]

    # Prepare the prompt

    prompt = f"""

    More information about Australian beaches at australia.com:

    {context}

    Question: {question}


    Extract the answer of the question from the text provided.

    If the text doesn't contain the answer,

    reply that the answer is not available."""



    prediction = co.generate(

        prompt=prompt,

        max_tokens=70,

        model="command-nightly",

        temperature=0.5,

        num_generations=num_generations

    )

    return prediction.generations

results = ask_llm(question,)

print(results[0])

question = "Where is the headquarters of the Congregation of the Holy Cross?"

results = ask_llm(question,)

print(results[0])

question = "The Basilica of the Sacred heart at Notre Dame is beside to which structure?"

results = ask_llm(question,)

print(results[0])



!pip install openai

import openai

!pip install --upgrade openai

import openai
from sklearn.metrics import accuracy_score

def evaluate_model(model, dataset):
    predicted_answers = []
    true_answers = []
    for question, true_answer in dataset:
        predicted_answer = model(question, num_generations=1)[0]  # Assuming ask_llm returns a list with the generated answers
        predicted_answers.append(predicted_answer)
        true_answers.append(true_answer)

    accuracy = accuracy_score(true_answers, predicted_answers)
    return accuracy

def ask_rag(question, num_generations=1):
    # Assuming you have the 'co' client initialized
    results = search_text(question)
    context = results[0]

    response = openai.Completion.create(
        model="openai/rag-davinci-003",  # Use the RAG model
        documents=context,
        question=question,
        max_tokens=70,
        n=1,  # Number of completions to generate
        stop=None,
        temperature=0.5
    )

    return [choice['text'].strip() for choice in response.choices]

def evaluate_until_threshold(model, dataset, threshold=0.8, max_iterations=10):
    current_accuracy = 0
    iteration = 0
    while current_accuracy < threshold and iteration < max_iterations:
        current_accuracy = evaluate_model(model, dataset)
        print(f"Iteration {iteration + 1}: Accuracy = {current_accuracy}")
        iteration += 1

    if current_accuracy < threshold:
        print(f"Unable to achieve accuracy > {threshold} after {max_iterations} iterations.")
    else:
        print(f"Target accuracy of {threshold} achieved after {iteration} iterations.")

# Example labeled dataset
labeled_dataset = [
    ("Where is the headquarters of the Congregation of the Holy Cross?", "Rome"),
    # Add more questions and answers as needed
]

# Evaluate the model until the accuracy reaches 80%
evaluate_until_threshold(ask_rag, labeled_dataset, threshold=0.8)



















